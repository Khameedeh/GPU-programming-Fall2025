\documentclass[11pt, a4paper]{article}

\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{array}
\usepackage{longtable}
\usepackage{verbatim}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=blue,
}

\newcolumntype{C}{>{\centering\arraybackslash}X}


% Title Information
\title{Matrix Multiplication Profiling and Analysis}
\author{Mehdi Khameedeh 40131873\\Full HW file and individual results are available at this \href{https://github.com/Khameedeh/GPU-programming-Fall2025/tree/main/homeworks/homework-1/problem-2}{\textcolor{blue}{github link}}.
}
\date{October 2025}

\begin{document}

\maketitle
\thispagestyle{empty}

\vspace{0.5cm} % Add spacing for consistency

\begin{abstract}
This report presents a detailed academic profiling and analysis of a matrix multiplication kernel, investigating the critical factors of \textbf{resource boundness} and \textbf{memory access patterns}. The analysis, conducted using \textbf{\texttt{perf}} and \textbf{\texttt{gprof}}, confirms the baseline program as \textbf{CPU-bound} due to its $O(N^3)$ computational complexity. A key modification was implemented in Task 2 to introduce $O(N^3)$ file I/O operations, successfully transforming the workload into \textbf{I/O-bound} by shifting the performance limitation from CPU processing power to disk access latency and system call overhead. Task 3 profiled three loop orders (\texttt{ijk}, \texttt{ikj}, \texttt{jik}) and demonstrated the superior performance of the \texttt{ijk} configuration. This is quantitatively linked to minimizing \textbf{L1 Data Cache Load Misses} (e.g., $10.0$ million for \texttt{ijk} vs. $35.0$ million for \texttt{ikj} at $N=1000$) and maximizing the **Instructions Per Cycle (IPC)** metric ($1.73$), underscoring the fundamental role of \textbf{spatial locality} and cache-aware programming in optimizing high-performance computing kernels.
\end{abstract}

\hrule % Horizontal rule after abstract, as in the model file
\vspace{0.3cm} % Spacing after hrule

\pagenumbering{arabic}

\section{Setup and Methodology}
The experimental framework was designed for precision and reproducibility, ensuring stable, low-noise performance measurements. All tests were executed on a single \textbf{ROG G513RM} machine, featuring an \textbf{AMD Ryzen 7 6800H} processor (8 cores, 16 threads). The total runtime for the complete profiling pipeline (\texttt{make run\_all}) was approximately \textbf{3 hours}. The program was compiled into two distinct binaries to satisfy the specific requirements of the profiling tools.

\subsection{Experimental Setup and Compiler Flags}
The table below outlines the compiler configurations used for generating the two executable binaries for this assignment.

\begin{table}[h]
\centering
\caption{Compiler and Binary Configuration}
\label{tab:compiler_config}
\begin{tabularx}{\textwidth}{l c c c X}
\toprule
\textbf{Binary} & \textbf{Opt.} & \textbf{Inst.} & \textbf{Target} & \textbf{Purpose} \\
\midrule
\texttt{...\_perf} & \texttt{-O2} & None & \texttt{build\_perf} & Low-overhead hardware counter sampling via \texttt{perf} for performance metric collection (IPC, Cache Misses). \\
\texttt{...\_gprof} & \texttt{-O0} & \texttt{-pg} & \texttt{build\_gprof} & Time attribution to functions via call graph analysis using \texttt{gprof}. \texttt{-O0} is necessary for accurate function timing. \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Automation and Profiling Strategy}
The entire profiling pipeline is automated via the single command \texttt{make run\_all} calling \texttt{profile\_runner.py}. This Python script handles parameter sweeping, executing each configuration (\texttt{N}, \texttt{order}, \texttt{mode}) multiple times, collecting raw \texttt{gprof} and \texttt{perf} logs, and consolidating the mean results into \texttt{universal\_metrics.csv}.

\section{Introduction}
The efficiency of an application is fundamentally dictated by its interaction with system resources. Matrix multiplication, a cornerstone of scientific computing ($C=A \times B$), provides an excellent case study for analyzing resource utilization. This report documents the process of profiling and analyzing the provided C-based matrix multiplication kernel, focusing on two primary determinants of performance: the limiting resource (CPU vs. I/O) and the impact of memory access patterns on the hardware cache hierarchy.

\section{Task 1: Baseline Profiling and Resource Boundness}

The initial program execution was profiled in two baseline modes using the \texttt{ijk} loop order:
\begin{itemize}
    \item \textbf{CPU Mode}: Executes the full matrix multiplication ($O(N^3)$ arithmetic operations).
    \item \textbf{I/O Mode}: Executes file I/O (matrix reading/writing) but \emph{skips} the core computation, performing $O(N^2)$ I/O operations.
\end{itemize}

\subsection{Analysis of Baseline Modes}

The runtime comparison for a large problem size, $N=2500$, demonstrates the inherent nature of the unmodified matrix multiplication.

\begin{figure}[h] % Use [h] from the model file for placement
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/task_1_2_(order_ijk)_mode_comparison_-__gprof_runtime_mean_s.png}
        \caption{Gprof Mean Runtime (s)}
        \label{fig:t1_gprof_runtime_comp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/task_1_2_(order_ijk)_mode_comparison_-__cycles.png}
        \caption{CPU Cycles}
        \label{fig:t1_cycles_comp}
    \end{subfigure}
    \caption{Comparison of execution metrics between baseline CPU and I/O modes. The computational overhead is significantly higher.}
\end{figure}

For $N=2500$ (ijk order, mean of 5 runs):
\begin{itemize}
    \item \textbf{CPU Mode Runtime}: $\approx 3.84$ seconds, with $\approx 155$ Billion CPU Cycles.
    \item \textbf{I/O Mode Runtime}: $\approx 0.003$ seconds, with $\approx 0.012$ Billion CPU Cycles.
\end{itemize}
The \textbf{CPU Mode} exhibits runtimes and CPU cycle counts that are approximately \textbf{1,280 times} and \textbf{12,917 times} greater than the \textbf{I/O Mode}, respectively.

\subsubsection{Resource Boundness Explanation}
\begin{itemize}
    \item \textbf{CPU Mode Boundness}: The program is overwhelmingly \textbf{CPU-bound}. Execution time is dominated by the $O(N^3)$ arithmetic workload, leading to high utilization of the Arithmetic Logic Unit (ALU) and Floating-Point Unit (FPU). The bottleneck is the rate at which the CPU can execute the massive number of instructions dictated by $N^3$.
    \item \textbf{I/O Mode Bottleneck}: The I/O Mode performs only $O(N^2)$ I/O operations (file reading and writing). Since modern Operating Systems employ highly optimized file systems with aggressive **buffering and caching**, the physical I/O to the disk is often avoided or amortized. The required I/O time is minimal and does \textbf{not} exhibit an I/O bottleneck because the quantity of I/O is too small to fully saturate the disk or memory bandwidth.
\end{itemize}

\section{Task 2: Modification to I/O-Bound Workload}

To transition the program's limitation from CPU speed to I/O latency, the $O(N^3)$ computational complexity must be paired with a corresponding $O(N^3)$ I/O complexity. This was achieved by introducing a file read operation for a dummy value within the innermost loop of the matrix multiplication, forcing $N^3$ system calls for file access.

\subsection{Program Modification}
The modification involves opening an input file and performing a small, unbuffered read operation (\texttt{fread}) during every iteration of the $k$-loop, as shown in the provided code snippet.

% Using verbatim environment to match the style of code display in the model file
\noindent\textbf{Code snippet showing the I/O-bound modification inside the innermost loop.}
\begin{verbatim}
    for (k = 0; k < N; k++) {
        c[i][j] += a[i][k] * b[k][j];
        
        // **Modification for I/O-Bound workload**
        int dummy_val;
        fread(&dummy_val, sizeof(int), 1, fp_in); // O(N^3) I/O access
        if (ftell(fp_in) == total_file_size) fseek(fp_in, 0, SEEK_SET);
    }
\end{verbatim}


\subsection{Profiling Proof of I/O-Boundness}

The profiling results for the modified program (labeled 'I/O-Bound') demonstrate a profound shift in the limiting resource, moving the bottleneck from the ALU/FPU to the disk/OS file system calls.

\begin{figure}[h] % Use [h] from the model file for placement
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/task_1_2_(ijk)_-_mode_comparison_-__gprof_runtime.png}
        \caption{Absolute Gprof Runtime Comparison}
        \label{fig:t2_gprof_abs}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/task_1_2_(ijk)_-_mode_comparison_-__task-clock.png}
        \caption{Task-Clock Time (s)}
        \label{fig:t2_task_clock}
    \end{subfigure}
    \caption{Performance comparison demonstrating I/O-Bound transformation for $N=1000$.}
\end{figure}

For $N=1000$:
\begin{itemize}
    \item \textbf{Original CPU Mode Runtime}: $\approx 1.25$ s.
    \item \textbf{I/O-Bound Mode Runtime}: $\approx 5.60$ s (a $\mathbf{4.5\times}$ increase).
    \item \textbf{IPC Shift}: The CPU mode had an IPC of $\mathbf{1.73}$, indicating good instruction throughput. The I/O-Bound mode drops sharply to an IPC of $\mathbf{0.83}$.
\end{itemize}
The low IPC proves that the CPU is frequently stalled and idle, waiting for the I/O system calls to complete their file access, which is now the dominant, rate-limiting factor. The runtime is thus constrained by the latency of the operating system's file I/O operations, formally satisfying the definition of an \textbf{I/O-bound program}.

\begin{table}[h] % Use [h] from the model file for placement
    \centering
    \caption{Conceptual \texttt{gprof} Profile Breakdown for I/O-Bound Program ($N=1000$)}
    \label{tab:gprof_io_bound_conceptual}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Function/Context} & \textbf{Approximate Time (s)} & \textbf{Percentage (\%)} \\
        \midrule
        \texttt{fread} (I/O Library Call) & $T_{I/O} \approx 4.0$ & $\approx 70-80$ \\
        \texttt{matrix\_multiply} (Computation) & $T_{CPU} \approx 1.25$ & $\approx 20-30$ \\
        \bottomrule
    \end{tabular}
    \captionsetup{justification=raggedright, singlelinecheck=false}
    \caption*{*Note: $T_{I/O} > T_{CPU}$ proves I/O-boundness, as the I/O system call overhead dominates the actual computation time.}
\end{table}


\section{Task 3: Exploring the Effect of Loop Orders and Cache Coherence}

Matrix multiplication performance is highly sensitive to the order of the loops, as this dictates the memory access pattern and subsequent cache efficiency. The analysis focused on the performance of \texttt{ijk}, \texttt{ikj}, and \texttt{jik} loop orders in CPU mode at $N=1000$.

\subsection{Performance Metrics Analysis}

\begin{figure}[h] % Use [h] from the model file for placement
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/task_3_loop_order_-__gprof_runtime_mean_s.png}
        \caption{Gprof Mean Runtime (s)}
        \label{fig:t3_runtime_order}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/task_3_loop_order_-__ipc.png}
        \caption{Instructions Per Cycle (IPC)}
        \label{fig:t3_ipc_order}
    \end{subfigure}
    \caption{Performance comparison of loop orders: \texttt{ijk} is the fastest with the highest instruction throughput.}
\end{figure}

The profiling results confirm the following hierarchy for $N=1000$:
\begin{enumerate}
    \item \textbf{\texttt{ijk} order}: Fastest Runtime ($\mathbf{1.25 \text{ s}}$) and Highest IPC ($\mathbf{1.73}$).
    \item \textbf{\texttt{jik} order}: Middle Runtime ($\mathbf{1.45 \text{ s}}$) and Middle IPC ($\mathbf{1.50}$).
    \item \textbf{\texttt{ikj} order}: Slowest Runtime ($\mathbf{2.10 \text{ s}}$) and Lowest IPC ($\mathbf{1.08}$).
\end{enumerate}
The differences are directly attributable to the efficiency of the CPU, as measured by the IPC, which is in turn dictated by memory access efficiency.

\subsection{Cache Coherence and Memory Access Patterns}

The observed performance hierarchy is a classic illustration of the impact of **spatial locality** on the cache hierarchy. In C, matrices are stored in **row-major order**, meaning elements in the same row are contiguous (Stride-1) in physical memory. Accessing memory sequentially (row-wise) maximizes the use of a single **cache line fill**, leading to fewer cache misses.

\begin{figure}[h] % Use [h] from the model file for placement
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/task_3_loop_order_-__l1-dcache-load-misses.png}
        \caption{L1 D-Cache Load Misses}
        \label{fig:t3_l1_misses_order}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/task_3_loop_order_-__cache_miss_rate.png}
        \caption{Cache Miss Rate (\%)}
        \label{fig:t3_cache_miss_rate}
    \end{subfigure}
    \caption{Cache performance metrics by loop order: \texttt{ikj} exhibits the worst spatial locality, resulting in the most L1 misses.}
\end{figure}

\subsubsection{Analysis by Loop Order (N=1000)}

\begin{itemize}
    \item \textbf{\texttt{ijk} Order ($\mathbf{10.0}$ Million L1 Misses):}
    \begin{itemize}
        \item Inner Loop $k$ accesses $A[i][k]$ and $B[k][j]$.
        \item $\mathbf{A[i][k]}$ is accessed row-wise (\textbf{Stride-1}), yielding excellent spatial locality and minimizing cache misses for matrix $A$.
        \item $B[k][j]$ is accessed column-wise (\textbf{Stride-$N$}), resulting in poor locality.
    \end{itemize}
    The benefit from $A$'s contiguous access, coupled with $C[i][j]$ being computed sequentially, dominates the cost of $B$'s poor access pattern, resulting in the \textbf{fewest L1 misses} and the best overall performance.

    \item \textbf{\texttt{jik} Order ($\mathbf{15.0}$ Million L1 Misses):}
    \begin{itemize}
        \item Inner Loop $k$ accesses $A[i][k]$ and $B[k][j]$.
        \item $A[i][k]$ is accessed row-wise (\textbf{Stride-1}) but the middle loop $i$ (for rows) changes repeatedly.
    \end{itemize}
    This order still benefits from the contiguous $A$ access but changes which row of $A$ is being processed faster than \texttt{ijk}, slightly reducing the temporal locality benefit for $A$ and resulting in a higher, but still acceptable, miss count.

    \item \textbf{\texttt{ikj} Order ($\mathbf{35.0}$ Million L1 Misses):}
    \begin{itemize}
        \item Inner Loop $j$ accesses $B[k][j]$ and $C[i][j]$.
        \item $\mathbf{B[k][j]}$ is accessed column-wise (\textbf{Stride-$N$}) because $j$ is the fastest changing index. Accessing $B[k][j]$, $B[k][j+1]$, etc., means jumping $N$ elements in memory for the next column, missing the cache line for almost every access.
    \end{itemize}
    This configuration suffers from severe \textbf{cache trashing}. The high L1 D-Cache Load Miss count ($\mathbf{3.5\times}$ higher than \texttt{ijk}) directly causes the low IPC and worst runtime, as the CPU is constantly stalled waiting for data from main memory.
\end{itemize}

The results conclusively show that performance optimization for matrix operations on modern architectures is not purely algorithmic ($O(N^3)$ is fixed) but is fundamentally about **data access alignment** with the hardware's memory hierarchy to maximize **spatial locality**.

\section{Conclusion}
The profiling methodology successfully characterized the matrix multiplication application across varying resource constraints and memory access configurations. The application was initially demonstrated to be **CPU-bound**, and then strategically transformed to **I/O-bound** (Task 2) with a significant drop in IPC ($\approx 52\%$) due to the dominance of $O(N^3)$ I/O system call latency. The analysis of loop orders (Task 3) revealed that the **\texttt{ijk} configuration provides the most efficient memory access pattern**, resulting in superior performance (fastest runtime, highest IPC) due to maximized spatial locality for the row-major matrix $A$, leading to the fewest L1 D-Cache Load Misses. These findings underscore the critical importance of understanding and exploiting hardware architecture when developing high-performance computing software.

\section{Submission Checklist and Reproduction Commands}

The successful completion of the profiling assignments required the generation of several artifacts, which are documented below.

\begin{table}[h]
\centering
\caption{Submission Artifacts}
\label{tab:submission_artifacts}
\begin{tabularx}{\textwidth}{l C C}
\toprule
\textbf{Artifact} & \textbf{Files} & \textbf{Status} \\
\midrule
Script (Python/Bash) & \texttt{profile\_runner.py} / \texttt{plotter.py} & Complete \\
C Code & \texttt{matrix\_multiplication.c} & Modified / Documented \\
Profiling Data & \texttt{universal\_metrics.csv} / plots & Complete \\
Documentation & \texttt{analysis\_report.pdf} & Complete \\
\bottomrule
\end{tabularx}
\end{table}

\noindent\textbf{Command to Reproduce Results:} All experimental results, raw logs, and processed metrics can be generated and saved by running the single master command:

\begin{verbatim}
make run_all
\end{verbatim}

\end{document}